{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f62d6c8f-c86f-43e0-b54c-f259be0fc80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/adityapadekar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/adityapadekar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/adityapadekar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/adityapadekar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math, re, nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7233aed-4d94-4f7f-9eb8-e6a85b50ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\n",
    "doc2 = \"\"\n",
    "with open('doc_01.txt', 'r') as file:\n",
    "    doc = file.read()\n",
    "\n",
    "with open('doc_02.txt', 'r') as file:\n",
    "    doc2 = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef47c9-1e20-4456-8f71-06a052fd775a",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "Tokenization is the process of breaking down a text into individual words or tokens. This is often the first step in natural language processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9dd946e-f0f4-48f2-aaab-33b3517ae552",
   "metadata": {},
   "outputs": [],
   "source": [
    "words= nltk.word_tokenize(doc)\n",
    "sentences = nltk.sent_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c993ec-1cfd-47d2-885f-07390d518309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Aditya', 'and', 'I', 'study', 'in', 'DYPCOE', '.', 'I', 'am', 'studying', 'computer', 'engineering', 'and', 'I', 'am', 'in', 'my', '3rd', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7574c244-3b2a-4592-bab9-6e6961d9560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi my name is Aditya and I study in DYPCOE.', 'I am studying computer engineering and I am in my 3rd year.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41de5daf-2f48-4b42-8cdf-b91e1d782fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pos_tag = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3b4bc0-a846-4d76-9935-a4ec4c0cd650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NNP'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Aditya', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('study', 'VBP'), ('in', 'IN'), ('DYPCOE', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('studying', 'VBG'), ('computer', 'NN'), ('engineering', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('in', 'IN'), ('my', 'PRP$'), ('3rd', 'CD'), ('year', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(word_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f76e5e-6608-40e0-a44a-e8aba833a14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 'those', 'wouldn', 'yours', 'its', 'same', 'by', \"wouldn't\", 'yourself', 'haven', 'myself', \"that'll\", 'so', 'just', 'in', 'these', 'not', 'as', 'an', \"didn't\", 'be', 'will', \"shan't\", \"mightn't\", 'some', \"aren't\", 'all', 'your', 'them', 'nor', 'they', 'should', 'and', 'while', 'his', 'at', 'hadn', 'few', 'are', 'doing', 'but', \"you'd\", 'have', 'aren', 'into', 've', 'of', 'on', 'yourselves', 'was', 'there', 'when', 'off', 'both', 'now', \"you'll\", 'am', 'd', 'mustn', 'doesn', 'how', 'through', 'each', 'it', 'he', 'over', \"won't\", 'during', \"isn't\", 'until', 'very', 'before', 'were', 'didn', 'won', 'him', 'their', 'can', 'does', 'couldn', \"it's\", 'itself', 'which', 'then', \"doesn't\", 'who', 'ours', 'other', 'her', 'theirs', 'such', 'herself', 'being', 'again', 't', 'don', 'had', 'me', \"couldn't\", 'i', \"needn't\", \"wasn't\", 'here', 'the', 'own', 'out', 'against', 'below', \"haven't\", 'himself', 'ourselves', \"hadn't\", 'about', \"you've\", 'further', 'wasn', \"you're\", 'hers', 'll', 'shouldn', 'above', 'themselves', 'mightn', 'under', 'no', \"hasn't\", 'weren', 'more', 'why', \"don't\", 'than', 'too', 'shan', 'a', 'is', 'after', 'down', 'needn', 'only', 'she', 'do', \"weren't\", 'been', 'has', 'any', 'with', 'we', 'from', 'you', 'once', 'ain', 'm', 're', 'hasn', 'o', 'my', 'having', 'or', 'most', 'ma', \"should've\", 'up', 'because', \"shouldn't\", 'to', 's', 'y', 'what', 'if', 'whom', 'our', 'for', 'where', \"mustn't\", \"she's\", 'did', 'that', 'between', 'this'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65dfe01d-bacb-4894-8f8c-16d8be31d836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'name', 'Aditya', 'study', 'DYPCOE', '.', 'studying', 'computer', 'engineering', '3rd', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [token for token in words if token.lower() not in stop_words]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "749228a9-66f5-4b14-b900-24c5b7116e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Aditya', 'and', 'I', 'study', 'in', 'DYPCOE', '.', 'I', 'am', 'studying', 'computer', 'engineering', 'and', 'I', 'am', 'in', 'my', '3rd', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in words]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d5deed9-570d-4fcc-8b6a-f3c9937174c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'name', 'aditya', 'studi', 'dypco', '.', 'studi', 'comput', 'engin', '3rd', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in word_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9be89e2-4bb7-4567-94e1-c60bafb35842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_freq(doc):\n",
    "    word_tokens = nltk.word_tokenize(doc)\n",
    "    tf_dict = dict()\n",
    "    for word in word_tokens:\n",
    "        tf_dict[word] = word_tokens.count(word)\n",
    "    tf = dict()\n",
    "    for word, count in tf_dict.items():\n",
    "        tf[word] = count/len(tf_dict)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3133e40c-ff2a-4c14-ab99-dcaf64ef368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of Doc 1:  {'Hi': 0.058823529411764705, 'my': 0.11764705882352941, 'name': 0.058823529411764705, 'is': 0.058823529411764705, 'Aditya': 0.058823529411764705, 'and': 0.11764705882352941, 'I': 0.17647058823529413, 'study': 0.058823529411764705, 'in': 0.11764705882352941, 'DYPCOE': 0.058823529411764705, '.': 0.11764705882352941, 'am': 0.11764705882352941, 'studying': 0.058823529411764705, 'computer': 0.058823529411764705, 'engineering': 0.058823529411764705, '3rd': 0.058823529411764705, 'year': 0.058823529411764705} \n",
      "\n",
      "\n",
      "Term Frequency of Doc 2:  {'Hi': 0.06666666666666667, 'how': 0.06666666666666667, 'are': 0.06666666666666667, 'you': 0.06666666666666667, '.': 0.2, 'I': 0.13333333333333333, 'would': 0.06666666666666667, 'like': 0.06666666666666667, 'to': 0.13333333333333333, 'say': 0.13333333333333333, 'something': 0.06666666666666667, 'do': 0.06666666666666667, \"n't\": 0.06666666666666667, 'want': 0.06666666666666667, 'anything': 0.06666666666666667}\n"
     ]
    }
   ],
   "source": [
    "tf = calculate_term_freq(doc)\n",
    "tf2 = calculate_term_freq(doc2)\n",
    "\n",
    "print(\"Term Frequency of Doc 1: \", tf, \"\\n\\n\")\n",
    "print(\"Term Frequency of Doc 2: \", tf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf17245-5d15-40c1-8930-da3d4af43534",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_tokens = nltk.word_tokenize(doc)\n",
    "d2_tokens = nltk.word_tokenize(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "492ff7ab-c956-425e-a47e-31a4fb5f7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf():\n",
    "    N = 2\n",
    "    all_tokens = d1_tokens + d2_tokens\n",
    "    idf = dict()\n",
    "    for token in all_tokens:\n",
    "        f = 0                   # Changed f from 1 back to 0\n",
    "        if token in d1_tokens:\n",
    "            f+=1\n",
    "        if token in d2_tokens:\n",
    "            f+=1\n",
    "        idf[token] = math.log(N/f)\n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f63a3b2c-7574-4be0-a626-d38c5ed29e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = calculate_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ef5f2d9-1649-4a8d-bc0c-0958c41577ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hi': 0.0,\n",
       " 'my': 0.6931471805599453,\n",
       " 'name': 0.6931471805599453,\n",
       " 'is': 0.6931471805599453,\n",
       " 'Aditya': 0.6931471805599453,\n",
       " 'and': 0.6931471805599453,\n",
       " 'I': 0.0,\n",
       " 'study': 0.6931471805599453,\n",
       " 'in': 0.6931471805599453,\n",
       " 'DYPCOE': 0.6931471805599453,\n",
       " '.': 0.0,\n",
       " 'am': 0.6931471805599453,\n",
       " 'studying': 0.6931471805599453,\n",
       " 'computer': 0.6931471805599453,\n",
       " 'engineering': 0.6931471805599453,\n",
       " '3rd': 0.6931471805599453,\n",
       " 'year': 0.6931471805599453,\n",
       " 'how': 0.6931471805599453,\n",
       " 'are': 0.6931471805599453,\n",
       " 'you': 0.6931471805599453,\n",
       " 'would': 0.6931471805599453,\n",
       " 'like': 0.6931471805599453,\n",
       " 'to': 0.6931471805599453,\n",
       " 'say': 0.6931471805599453,\n",
       " 'something': 0.6931471805599453,\n",
       " 'do': 0.6931471805599453,\n",
       " \"n't\": 0.6931471805599453,\n",
       " 'want': 0.6931471805599453,\n",
       " 'anything': 0.6931471805599453}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83d0067b-5680-4efd-a03f-888223ef8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_repr = []\n",
    "for token in d1_tokens:\n",
    "    d1_repr.append(tf[token] * idf[token])\n",
    "d2_repr = []\n",
    "for token in d2_tokens:\n",
    "    d2_repr.append(tf2[token] * idf[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e94a0391-d2a4-4c6b-b1e4-7b112c687ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.08154672712469944, 0.04077336356234972, 0.04077336356234972, 0.04077336356234972, 0.08154672712469944, 0.0, 0.04077336356234972, 0.08154672712469944, 0.04077336356234972, 0.0, 0.0, 0.08154672712469944, 0.04077336356234972, 0.04077336356234972, 0.04077336356234972, 0.08154672712469944, 0.0, 0.08154672712469944, 0.08154672712469944, 0.08154672712469944, 0.04077336356234972, 0.04077336356234972, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(d1_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21b5dd14-6f97-4623-8a6d-279fca029625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.046209812037329684, 0.046209812037329684, 0.046209812037329684, 0.0, 0.0, 0.046209812037329684, 0.046209812037329684, 0.09241962407465937, 0.09241962407465937, 0.046209812037329684, 0.0, 0.0, 0.046209812037329684, 0.046209812037329684, 0.046209812037329684, 0.09241962407465937, 0.09241962407465937, 0.046209812037329684, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(d2_repr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
